---
title: "IMDB Movie Reviews Classification"
format: html
excute:
  warning: false
  echo: true
  message: false
  fig-width: 8
  fig-height: 6
  dpi: 180
---

# Overview of the Project
In this NLP project I will be attempting to classify `IMDB Movie Reviews` into a positive and negative sentiment categories based off the text content in the reviews. This dataset is publicly available at [kaggle](https://www.kaggle.com/datasets).

Each observation in this dataset is a review of movies. The `sentiment` column is the category of those reviews as positive and negative, and `review` column is the text review by customers.

# Data set preparation and pre-processing
*I downloaded the dataset from Kaggle to my personal computer and read the `IMDB Dataset.csv` to the project environment.*

## Load Libraires

```{r}
library(tidyverse) # general data preparation
library(tidytext) # text modeling
library(tidylo) # words log odds ratio
library(tvthemes) # tweak default ggplot2 themes and font styles
library(textfeatures) # for text features
theme_set(theme_light())# theme
library(tidymodels) # tidyverse ecosystem for building models

```
# Import Data

```{r, echo=TRUE, warning=FALSE, message=FALSE}

imdb_review <- read_csv("IMDB Dataset.csv")

```

*Let's check the head, info, describe methods on imdb_review data.*

```{r}

head(imdb_review)
glimpse(imdb_review)
summary(imdb_review)

```
*Let's create a new column called `Review Length` which is the number of words in the `review` column.*

```{r}
imdb_review <- imdb_review %>% 
  mutate(`Review Length` = nchar(review))
```

**Use `group_by` to get the mean values for the `Review Length` by `sentiment`.**
```{r}
imdb_review %>% 
  group_by(sentiment) %>% 
  summarise(AvergeWord = mean(`Review Length`))
```
***Average words in the positive sentiment is larger than the negative sentiment.***

## Explanatory Data Analysis 
I will use the `facet_wrap` function from `ggplot2` library to create a grid of 2 histograms of review text length based off the sentiments.
```{r}
imdb_review %>% 
  ggplot(aes(`Review Length`))+
  geom_histogram(bins = 30, fill = "lightblue", color = "black", show.legend = FALSE)+
  facet_wrap(~sentiment)
```
**Create a boxplot of `Review Length` for each `sentiment` category.**

```{r}
imdb_review %>% 
  ggplot(aes(x = sentiment, y = `Review Length`, fill = sentiment))+
  geom_boxplot(show.legend = FALSE)
```
Let's the `log odd ration` of the words bind sentiments and words.
```{r}
imdb_lo <- imdb_review %>% 
  unnest_tokens(word, review) %>% 
  count(sentiment, word) %>% 
  bind_log_odds(sentiment, word, n) %>% 
  arrange(-log_odds_weighted)
  
imdb_lo %>% 
  group_by(sentiment) %>% 
  top_n(20) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, log_odds_weighted)) %>% 
  ggplot(aes(log_odds_weighted, word , fill = sentiment))+
  geom_col(alpha = 0.8, show.legend = FALSE)+
  facet_wrap(~sentiment, scales = "free")+
  labs(x = "Log Odds Weighted")+
  scale_fill_avatar(palette = "WaterTribe")
```

The above words make sense for the positive and negative sentiment of the movie
reviews. The data frame contains some `html` tags and I would like to see how
this is structured. I will use the `textfeatures` package for this task. And
let's explore by creating text features from review column.

# NLP Classification Task 

***Let's move on the actual task. I will use the `textfeatures` package to get extra features for the `review` text.`textfeatures` requires a column that needs to be transformed the name would would be changed to `text`.***


```{r}

# text-feature looking one column to be named as text  
text_feature <- textfeatures(
  imdb_review %>% mutate(text = review), 
  sentiment = FALSE,
  word_dims = 0,
  normalize = FALSE,
  verbose = FALSE)

text_feature %>% 
  bind_cols(imdb_review) %>% 
  group_by(sentiment) %>% 
  summarise(across(starts_with("n_"), mean)) %>% 
  pivot_longer(starts_with("n_"), names_to = "text_feature") %>% 
  filter(value > 0.01) %>% 
  mutate(text_feature = fct_reorder(text_feature, -value)) %>% 
  ggplot(aes(x = sentiment, y = value, fill = sentiment)) +
    geom_col(position = "dodge", alpha = 0.8, show.legend = FALSE)+
    facet_wrap(~text_feature, scales = "free", ncol = 6)
```

> This is very interesting with hash tags related reviews are negative sentiments.

# Build a model
### Data split
A dataset used for machine learning should be partitioned into three subsets ---
training, test, and validation sets.

I will split the data set `75%` for training and the remaining `25%` for test
dataset.

```{r}
set.seed(202306)
imdb_split <- initial_split(imdb_review)
imdb_train <- training(imdb_split)
imdb_test <- testing(imdb_split)

```

**Cross-validation**.
Cross-validation is the most commonly used tuning method. It entails splitting a
training dataset into ten equal parts (folds). A given model is trained on only
nine folds and then tested on the tenth one (the one previously left out).
Training continues until every fold is left aside and used for testing. As a
result of model performance measure, a specialist calculates a cross-validated
score for each set of hyperparameters. A data scientist trains models with
different sets of hyperparameters to define which model has the highest
prediction accuracy. The cross-validated score indicates average model
performance across ten hold-out folds. During this stage, a data scientist
trains numerous models to define which one of them provides the most accurate
predictions.

```{r}
set.seed(123)
imdb_folds <- vfold_cv(imdb_train)
imdb_folds
```


> Next, let's prepocess out data to get it ready for modeling.

```{r}

library(textrecipes)
library(themis)

imdb_rec <- recipe(sentiment ~ review, data = imdb_train) %>% 
  step_textfeature(review) %>% # create text features using a step from the textreceip package
  step_zv(all_predictors()) %>%  # filter for zero variance all predictors
  step_normalize(all_predictors()) # center and scale all predictor variables
imdb_prep <- prep(imdb_rec)
imdb_prep
```

```{r}
juice(imdb_prep) # extract transformed training set
names(juice(imdb_prep)) # see the freatuers created by the step_textfearures
```

### Model training

After a data scientist has preprocessed the collected data and split it into
subsets, he or she can proceed with a model training. This process entails
"feeding" the algorithm with training data. An algorithm will process data and
output a model that is able to find a target value (attribute) in new data
an answer you want to get with predictive analysis. The purpose of model
training is to develop a model.

Let's start by creating the two different models, a random forest and
SVM(support vector machine).

```{r}
# model specifications

rf_spec <- rand_forest(trees = 1500) %>% 
  set_engine("ranger") %>% # set the engine ranger r package a core computational engine
  set_mode("classification") # set the type of model classification or regression
rf_spec

```

```{r}
svm_spec <- svm_rbf(cost = 0.5) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")

svm_spec
```

> Let's create workflow in tidymodels

```{r}
imdb_wf <- workflow() %>% 
  add_recipe(imdb_rec)

imdb_wf
```

Now we can add a model, and then fit to each of the resamples. First we can fit
the random forest model.

```{r}
doParallel::registerDoParallel()
set.seed(576)
rf_rs <- imdb_wf %>% 
  add_model(rf_spec) %>% 
  fit_resamples(
    resamples = imdb_folds,
    metrics = metric_set(roc_auc, accuracy, sens, spec),
    control = control_grid(save_pred = TRUE)
  )
```

> Next for the SVM model

```{r}

set.seed(908)
svm_rs <- imdb_wf %>% 
  add_model(svm_spec) %>% 
  fit_resamples(
    resamples = imdb_folds,
    metrics = metric_set(roc_auc, accuracy, sens, spec),
    control = control_grid(save_pred = TRUE)
  )

```

## Model evaluation and testing

The goal of this step is to develop the simplest model able to formulate a
target value fast and well enough. A data scientist can achieve this goal
through model tuning. That's the optimization of model parameters to achieve an
algorithm's best performance. One of the more efficient methods for model
evaluation and tuning is cross-validation.

Now let's evaluate our model

```{r}
collect_metrics(rf_rs)

```

```{r}
conf_mat_resampled(rf_rs)

```

```{r}
collect_metrics(svm_rs)
```

```{r}
conf_mat_resampled(svm_rs)
```

Almost two models have similar results. But SVM has better prediction for negative sentiments. 

```{r}
svm_rs %>% 
  collect_predictions() %>% 
  roc_curve(sentiment, .pred_negative) %>% 
  ggplot(aes(1-specificity, sensitivity))+
  geom_abline(lty = 2, color = "gray80", size = 1.5)+
  geom_path(show.legend = FALSE, alpha = 0.6, size = 1.2)+
  coord_equal()
```
Finally we can turn to the testing data to confirm that out performance is about the same.

```{r}
imdb_final <- imdb_wf %>% 
  add_model(svm_spec) %>% 
  last_fit(imdb_split)

imdb_final %>% 
  collect_metrics()

```
```{r}
imdb_final %>% 
  collect_predictions() %>% 
  conf_mat(sentiment, .pred_class)
```